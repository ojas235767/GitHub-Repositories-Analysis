# -*- coding: utf-8 -*-
"""GitHub-Repos-Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11oI_SzTLitXDyWFuzJ712_wzB_ZnUvvv

#Installed Packages
"""

!pip install requests beautifulsoup4 pandas seaborn matplotlib

"""#Create GitHub Scraper Module"""

with open('github_scraper.py', 'w') as f:
    f.write('''import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
from datetime import datetime
import time
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GitHubScraper:
    def __init__(self):
        self.base_url = "https://github.com/trending"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.raw_data = []

    def scrape_trending_repos(self, language="", since="daily"):
        try:
            url = f"{self.base_url}?since={since}"
            if language:
                url += f"&spoken_language_code=&d=1&l={language}"

            logger.info(f"Scraping URL: {url}")
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            repos = soup.find_all('article', class_='Box-row')

            logger.info(f"Found {len(repos)} repositories")

            for repo in repos:
                try:
                    repo_data = self._extract_repo_data(repo)
                    self.raw_data.append(repo_data)
                except Exception as e:
                    logger.warning(f"Error extracting repo data: {e}")
                    continue

            logger.info(f"Successfully scraped {len(self.raw_data)} repositories")
            return self.raw_data

        except requests.RequestException as e:
            logger.error(f"Request failed: {e}")
            return []

    def _extract_repo_data(self, repo):
        try:
            repo_link = repo.find('h2', class_='h3').find('a')
            repo_name = repo_link.text.strip().replace('\\n', '').replace(' ', '')
            repo_url = 'https://github.com' + repo_link.get('href', '').strip()

            description_elem = repo.find('p', class_='col-9')
            description = description_elem.text.strip() if description_elem else None

            lang_elem = repo.find('span', itemprop='programmingLanguage')
            language = lang_elem.text.strip() if lang_elem else None

            stars = None
            forks = None
            trending_stars = None

            repo_stats = repo.find_all('a', class_='Link--muted')
            for stat in repo_stats:
                text = stat.text.strip()
                if text and any(char.isdigit() for char in text):
                    try:
                        numeric_val = text.split()[0].replace(',', '').replace('k', '000')
                        if numeric_val.isdigit():
                            if 'star' in text.lower():
                                stars = numeric_val
                            elif 'fork' in text.lower():
                                forks = numeric_val
                    except:
                        pass

            if stars is None or forks is None:
                all_elements = repo.find_all(['span', 'a'])
                for elem in all_elements:
                    text = elem.text.strip()
                    if text and len(text) < 20 and any(char.isdigit() for char in text):
                        try:
                            numeric_val = text.split()[0].replace(',', '').replace('k', '000')
                            if numeric_val.isdigit():
                                numeric_val = str(int(numeric_val))
                                if stars is None:
                                    stars = numeric_val
                                elif forks is None and stars != numeric_val:
                                    forks = numeric_val
                        except:
                            pass

            trend_span = repo.find('span', class_='d-inline-block float-sm-right')
            if trend_span:
                trending_text = trend_span.text.strip()
                if trending_text:
                    try:
                        numeric_part = trending_text.split()[0].replace(',', '').replace('k', '000')
                        if numeric_part.isdigit():
                            trending_stars = numeric_part
                    except:
                        pass

            return {
                'repo_name': repo_name,
                'repo_url': repo_url,
                'description': description,
                'language': language,
                'stars': stars if stars else '0',
                'forks': forks if forks else '0',
                'trending_stars': trending_stars,
                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        except Exception as e:
            logger.error(f"Error extracting repo: {e}")
            raise

class DataQualityChecker:
    def __init__(self, df):
        self.df = df.copy()
        self.quality_report = {}

    def check_missing_values(self):
        missing = self.df.isnull().sum()
        self.quality_report['missing_values'] = missing[missing > 0].to_dict()
        logger.info(f"Missing values: {self.quality_report['missing_values']}")
        return self.quality_report['missing_values']

    def check_duplicates(self):
        duplicates = self.df[self.df.duplicated(subset=['repo_name'], keep=False)]
        self.quality_report['duplicate_count'] = len(duplicates)
        logger.info(f"Found {len(duplicates)} duplicate entries")
        return duplicates

    def check_data_types(self):
        try:
            self.df['stars'] = pd.to_numeric(self.df['stars'], errors='coerce')
            self.df['forks'] = pd.to_numeric(self.df['forks'], errors='coerce')
            self.df['trending_stars_numeric'] = self.df['trending_stars'].str.extract(r'(\\d+)').astype('float')
            self.quality_report['data_type_conversion'] = 'Success'
            logger.info("Data type conversion successful")
        except Exception as e:
            logger.error(f"Data type conversion failed: {e}")
            self.quality_report['data_type_conversion'] = f'Failed: {e}'

    def handle_missing_values(self):
        self.df['description'] = self.df['description'].fillna('Not Available')
        self.df['language'] = self.df['language'].fillna('Unknown')
        self.df['stars'] = self.df['stars'].fillna(0)
        self.df['forks'] = self.df['forks'].fillna(0)
        self.df['trending_stars_numeric'] = self.df['trending_stars_numeric'].fillna(0)
        logger.info("Missing values handled")

    def remove_duplicates(self):
        initial_count = len(self.df)
        self.df = self.df.drop_duplicates(subset=['repo_name'], keep='first')
        logger.info(f"Removed {initial_count - len(self.df)} duplicates")
        return self.df

    def validate_urls(self):
        self.df['url_valid'] = self.df['repo_url'].str.startswith('https://github.com/')
        invalid = self.df[~self.df['url_valid']]
        self.quality_report['invalid_urls'] = len(invalid)
        logger.info(f"Found {len(invalid)} invalid URLs")

    def check_data_quality_issues(self):
        suspicious = self.df[(self.df['stars'] == 0) & (self.df['trending_stars_numeric'] > 0)]
        self.quality_report['suspicious_zero_stars'] = len(suspicious)
        logger.warning(f"Found {len(suspicious)} repos with 0 stars but trending activity - possible extraction issue")
        return suspicious

    def get_cleaned_data(self):
        return self.df

    def generate_report(self):
        return self.quality_report

class DatabaseManager:
    def __init__(self, db_name='github_repos.db'):
        self.db_name = db_name
        self.conn = None
        self.cursor = None

    def connect(self):
        try:
            self.conn = sqlite3.connect(self.db_name)
            self.cursor = self.conn.cursor()
            logger.info(f"Connected to database: {self.db_name}")
        except sqlite3.Error as e:
            logger.error(f"Database connection failed: {e}")

    def create_tables(self):
        try:
            self.cursor.execute("""
                CREATE TABLE IF NOT EXISTS repositories (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    repo_name TEXT UNIQUE NOT NULL,
                    repo_url TEXT,
                    description TEXT,
                    language TEXT,
                    stars INTEGER,
                    forks INTEGER,
                    trending_stars_numeric REAL,
                    scraped_at TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            self.cursor.execute("""
                CREATE TABLE IF NOT EXISTS quality_checks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    check_type TEXT,
                    check_result TEXT,
                    checked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            self.conn.commit()
            logger.info("Tables created successfully")
        except sqlite3.Error as e:
            logger.error(f"Table creation failed: {e}")

    def insert_data(self, df):
        try:
            columns_to_insert = ['repo_name', 'repo_url', 'description', 'language', 'stars', 'forks', 'trending_stars_numeric', 'scraped_at']
            df_insert = df[columns_to_insert]
            df_insert.to_sql('repositories', self.conn, if_exists='append', index=False)
            self.conn.commit()
            logger.info(f"Inserted {len(df_insert)} records into database")
        except sqlite3.IntegrityError as e:
            logger.warning(f"Some records already exist: {e}")
            self.conn.commit()
        except Exception as e:
            logger.error(f"Error inserting data: {e}")

    def insert_quality_report(self, report):
        try:
            for check_type, result in report.items():
                self.cursor.execute("INSERT INTO quality_checks (check_type, check_result) VALUES (?, ?)", (check_type, str(result)))
            self.conn.commit()
            logger.info("Quality report inserted")
        except sqlite3.Error as e:
            logger.error(f"Failed to insert quality report: {e}")

    def fetch_data(self, query):
        try:
            return pd.read_sql_query(query, self.conn)
        except Exception as e:
            logger.error(f"Query failed: {e}")
            return pd.DataFrame()

    def close(self):
        if self.conn:
            self.conn.close()
            logger.info("Database connection closed")
''')

print("‚úÖ github_scraper.py created successfully!")

"""#Delete Old Database"""

import os

if os.path.exists('github_repos.db'):
    os.remove('github_repos.db')
    print("‚úÖ Old database deleted")
else:
    print("‚úÖ No old database found")

"""#Run Scraper & Database"""

import pandas as pd
from github_scraper import GitHubScraper, DataQualityChecker, DatabaseManager

# Scrape data
scraper = GitHubScraper()
raw_data = scraper.scrape_trending_repos(language="python", since="weekly")

# Convert to DataFrame
df = pd.DataFrame(raw_data)
print(f"Initial dataset: {df.shape}")

# Quality checks
quality_checker = DataQualityChecker(df)
quality_checker.check_missing_values()
quality_checker.check_duplicates()
quality_checker.check_data_types()
quality_checker.validate_urls()
quality_checker.check_data_quality_issues()

# Clean data
quality_checker.handle_missing_values()
quality_checker.remove_duplicates()
cleaned_df = quality_checker.get_cleaned_data()
print(f"Cleaned dataset: {cleaned_df.shape}")

# Save to database
db = DatabaseManager()
db.connect()
db.create_tables()
db.insert_data(cleaned_df)
db.insert_quality_report(quality_checker.generate_report())
db.close()

print("\n‚úÖ Scraping and database insertion complete!")

"""#View Top 10 Repositories"""

from github_scraper import DatabaseManager

db = DatabaseManager()
db.connect()
results = db.fetch_data('SELECT repo_name, language, stars, forks FROM repositories ORDER BY stars DESC LIMIT 10')
db.close()

print("\nüåü Top 10 Repositories by Stars:")
print(results.to_string())

"""# Create Professional Visualization"""

import matplotlib.pyplot as plt
import seaborn as sns
from github_scraper import DatabaseManager

sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 10)

db = DatabaseManager()
db.connect()
repos_df = db.fetch_data('SELECT * FROM repositories ORDER BY stars DESC LIMIT 25')
db.close()

# Create figure with subplots
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('GitHub Trending Repositories Analysis', fontsize=18, fontweight='bold', y=0.995)

# Subplot 1: Top 10 Repos by Stars
ax1 = axes[0, 0]
top_10 = repos_df.nlargest(10, 'stars')
bars = ax1.barh(top_10['repo_name'], top_10['stars'], color='steelblue', edgecolor='navy', alpha=0.8)
ax1.set_xlabel('Total Stars', fontsize=11, fontweight='bold')
ax1.set_title('Top 10 Repositories by Stars', fontsize=12, fontweight='bold')
ax1.invert_yaxis()
for i, bar in enumerate(bars):
    width = bar.get_width()
    ax1.text(width, bar.get_y() + bar.get_height()/2, f'{int(width):,}', ha='left', va='center', fontsize=9, fontweight='bold')

# Subplot 2: Stars vs Forks
ax2 = axes[0, 1]
scatter = ax2.scatter(repos_df['stars'], repos_df['forks'], s=repos_df['trending_stars_numeric']*2, alpha=0.6, c=repos_df['stars'], cmap='viridis', edgecolors='black', linewidth=0.5)
ax2.set_xlabel('Total Stars', fontsize=11, fontweight='bold')
ax2.set_ylabel('Total Forks', fontsize=11, fontweight='bold')
ax2.set_title('Stars vs Forks (bubble size = trending stars)', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3)
plt.colorbar(scatter, ax=ax2, label='Stars')

# Subplot 3: Language Distribution
ax3 = axes[1, 0]
language_counts = repos_df['language'].value_counts()
colors = sns.color_palette('husl', len(language_counts))
wedges, texts, autotexts = ax3.pie(language_counts, labels=language_counts.index, autopct='%1.1f%%', colors=colors, startangle=90, textprops={'fontsize': 10})
ax3.set_title('Programming Language Distribution', fontsize=12, fontweight='bold')
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontweight('bold')

# Subplot 4: Weekly Growth
ax4 = axes[1, 1]
top_trending = repos_df.nlargest(10, 'trending_stars_numeric')
bars2 = ax4.bar(range(len(top_trending)), top_trending['trending_stars_numeric'], color='coral', edgecolor='darkred', alpha=0.8)
ax4.set_xticks(range(len(top_trending)))
ax4.set_xticklabels([name.split('/')[-1] for name in top_trending['repo_name']], rotation=45, ha='right', fontsize=9)
ax4.set_ylabel('New Stars (This Week)', fontsize=11, fontweight='bold')
ax4.set_title('Top 10 Repositories by Weekly Growth', fontsize=12, fontweight='bold')
ax4.grid(True, axis='y', alpha=0.3)
for bar in bars2:
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height, f'{int(height):,}', ha='center', va='bottom', fontsize=9, fontweight='bold')

plt.tight_layout()
plt.savefig('github_analysis.png', dpi=300, bbox_inches='tight')
print("‚úÖ Visualization saved as 'github_analysis.png'")
plt.show()

# Summary Statistics
print("\n" + "="*60)
print("GITHUB REPOSITORIES ANALYSIS SUMMARY")
print("="*60)
print(f"Total Repositories: {len(repos_df)}")
print(f"Average Stars: {repos_df['stars'].mean():,.0f}")
print(f"Median Stars: {repos_df['stars'].median():,.0f}")
print(f"Highest Starred: {repos_df.iloc[0]['repo_name']} ({repos_df['stars'].iloc[0]:,} stars)")
print(f"Average Forks: {repos_df['forks'].mean():,.0f}")
print(f"Total Weekly Growth: {repos_df['trending_stars_numeric'].sum():,.0f} new stars")
print("\nLanguage Distribution:")
for lang, count in repos_df['language'].value_counts().items():
    print(f"  {lang}: {count} repositories")
print("="*60)

"""#Export Data as CSV"""

from github_scraper import DatabaseManager

db = DatabaseManager()
db.connect()
repos_df = db.fetch_data('SELECT * FROM repositories')
db.close()

repos_df.to_csv('github_repos.csv', index=False)
print("‚úÖ Data exported to github_repos.csv")

"""#Download Files from Colab"""

from google.colab import files
import os

print("üì• Checking available files...")
print(os.listdir())

print("\nüì• Downloading files...\n")

# Download visualization
try:
    files.download('github_analysis.png')
    print("‚úÖ github_analysis.png - Check your Downloads folder")
except:
    print("‚ùå github_analysis.png not found")

# Download CSV
try:
    files.download('github_repos.csv')
    print("‚úÖ github_repos.csv - Check your Downloads folder")
except:
    print("‚ùå github_repos.csv not found")

# Download database
try:
    files.download('github_repos.db')
    print("‚úÖ github_repos.db - Check your Downloads folder")
except:
    print("‚ùå github_repos.db not found")

print("\n‚úÖ Download complete! Check your Downloads folder")